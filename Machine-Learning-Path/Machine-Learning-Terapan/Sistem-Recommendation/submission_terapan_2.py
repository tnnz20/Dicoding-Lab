# -*- coding: utf-8 -*-
"""submission_terapan_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w8f4qMc3FuC1x_U3xiGXfz-isOPmBp_j

# **Data Diri**

---
(Peserta Kampus merdeka - Student Indepentdent)


Nama : Gusti Muhammad Aulia Nur Sulthan

Alamat :  Kabupaten Hulu Sungai Selatan, Kalimantan Selatan

SIB Id : M247R6216

SIB Email : M247R6216@dicoding.org	

SIB Group : M3

# Introduction
Proyek ini merupakan submission dari [Dicoding]('www.dicoding.com') dan pada proyek kedua ini disuruh membuat **Sistem Rekomendasi**.

## Download Package
"""

! pip install opendatasets

"""## Libraries
Library umum yang digunakan pada notebook 
"""

# Commented out IPython magic to ensure Python compatibility.
import opendatasets as od
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns
plt.style.use('seaborn')

"""## Data Understanding
![image](https://user-images.githubusercontent.com/76864673/140640142-57306111-f34e-4223-99a7-4d970aba3867.png)


Datasets ini di ambil dari [Kaggle](https://www.kaggle.com/sunilgautam/movielens), dalam datasets ini terdiri dari 5 file csv yaitu :

* README.txt
* links.csv
* ratings.csv
* movies.csv
* tags.csv

dalam proyek sistem rekomendasi kali ini saya hanya menggunakan 2 file yaitu : `movies.csv` & `ratings.csv`

### Load Datasets
"""

od.download('https://www.kaggle.com/sunilgautam/movielens') #27c6756f2debb886a1d3f567bc77fedf

"""### Read Datasets"""

movies = pd.read_csv('/content/movielens/ml-latest-small/movies.csv')
ratings = pd.read_csv('/content/movielens/ml-latest-small/ratings.csv')

"""### Univariate Exploratory Data Analysis

#### Variable _movies_ 
Keterangan :
* `movieId` = Id dari movie (int64)
* `title` = Judul dari movie beserta tahun rilisnya (object)
* `genres` = Genre dari movie (object)
"""

movies.head()

movies.info()

"""##### Check Missing value"""

movies.isna().sum()

"""##### Check Total data unique"""

movies.nunique()

"""##### Check Data duplicate"""

movies.duplicated().sum()

"""##### Visualisasi *Tahun* 
Pertama pisahkan tahun yang berada di kolom title ke dalam kolom baru
"""

# Deklarasi variable baru
movies_year = movies.copy()

# Extract Tahun di dalam title
movies_year['year'] = movies['title'].str.extract('(\d+)').astype(float)
movies_year.year.dropna(inplace=True)

"""Karena ada beberapa judul yang tidak memiliki tahun saya seleksi"""

x = movies_year[movies_year['year'] > 1000.0]
x.year.astype(int)
data = x

plt.figure(figsize=(15,8))
sns.histplot(data=data, x='year',bins=50)
plt.title('Distribusi Tahun rilis Movies', fontsize=30, pad=30)
plt.tight_layout()
plt.show()

"""> Terlihat pada grafik histogram di atas rata - rata movies rilis pada rentang tahun 1990 - 2000 ke atas

##### Visualisasi *Genre*
Dalam visualisasi genre langkah pertama yang dilakukan adalah memisahkan genre apa saja yang ada dalam movie ini, lalu membuat kolom baru pada data yang berisi genre.
"""

# membuat list untuk menampung genre
movies_genres = movies.copy()
genres=[]
for i in range(len(movies.genres)):
    for x in movies.genres[i].split('|'):
        if x not in genres:
            genres.append(x)
genres

# Tambah kolom genres ke dalam movies_genres
for x in genres:
    movies_genres[x] = 0

for i in range(len(movies.genres)):
    for x in movies.genres[i].split('|'):
        movies_genres[x][i]=1

movies_genres

view = movies_genres.iloc[:,3:].sum().reset_index()
view.columns = ['title','total']

plt.figure(figsize=(15,12))
sns.barplot(y='title',x='total',data=view)
plt.title('Total Genre', fontsize=30, pad=30)
plt.tight_layout()
plt.show()

"""> Terlihat pada gambar di atas genre `Drama` yang paling banyak dan diikuti oleh genre `Comedy` lalu ada beberapa movies yang tidak memiliki genre `no genres listed`

#### *ratings* Variable
Keterangan :
* `userId` = Id pengguna (int64)
* `movieId` = Id film yang di rating (int64)
* `rating` = Skor dari rating yang diberikan (float64)
* `timestamp` = Waktu saat rating diberikan (int64)
"""

print(f'Jumlah userId yang memberi rating : {ratings.userId.nunique()}')
print(f'Jumlah movieId yang diberi rating oleh user : {ratings.movieId.nunique()}')

ratings.head()

ratings.info()

"""##### Check Missing value"""

ratings.isna().sum()

"""##### Check Total data unique"""

ratings.nunique()

"""##### Check Data duplicate"""

ratings.duplicated().sum()

"""##### Check Total value rating"""

ratings.rating.value_counts()

"""> Dalam data ini rating diberikan dengan range 0-5 dengan skala 0.5

##### Visualisasi Rating movies tertinggi
pada visualisasi ini langkah pertama yang dilakukan adalah menggabungkan `rating` dengan `movies` lalu mendrop kolom yang tidak digunakan
"""

rating_movies = pd.merge( ratings, movies, on='movieId', how='inner')
rating_movies.drop(['timestamp','genres'],axis=1, inplace=True)
rating_movies

"""setelah itu membuat variable yang menampung total rating yang dimiliki setiap judul"""

rating_movies_count = rating_movies.groupby('title')['rating'].count()
rating_movies_count = pd.DataFrame(rating_movies_count).reset_index().rename(columns={'rating':'total_rating'})
rating_movies_count

data = rating_movies_count.sort_values(by ='total_rating')

plt.figure(figsize=(15,10))
sns.barplot(data=data.iloc[-10:,:], 
            y='title', x='total_rating',
            palette="Blues_d")
plt.title('Top 10 Rating Movies', pad=30, fontsize=30)
plt.tight_layout()
plt.show()

"""> Terlihat pada grafik diatas bahwa movies dengan judul `Forrest Gump (1994)` memiliki rating tertingi.

#### Total data
"""

print(f'Jumlah Data Movie {movies.shape[0]}, dan memiliki {movies.shape[1]} kolom')
print(f'Jumlah Data Rating {ratings.shape[0]}, dan memiliki {ratings.shape[1]} kolom')

"""## Data Preparation
Dalam persiapan data yang saya lakukan adalah sebagai berikut :
1. Memastikan tidak ada missing value
2. Sorting `ratings` value berdasarkan userId dan mengubah data tersebut ke dalam integer
3. Mendrop data duplicate
4. Mengabungkan data `ratings` dengan `movies` setelah itu seleksi value dalam attribut genres yang tidak mengandung np.nan
"""

# Deklarasi variable
movies = pd.read_csv('/content/movielens/ml-latest-small/movies.csv')
ratings = pd.read_csv('/content/movielens/ml-latest-small/ratings.csv')

# Drop missing value
movies.dropna(axis=0, inplace=True)
ratings.dropna(axis=0, inplace=True)
print('Data sudah bersih dari missing value')

# Sorting ratings
ratings = ratings.sort_values('userId').astype('int')

# Drop Duplicate data
movies.drop_duplicates(subset=['title'], keep='first', inplace=True)
ratings.drop_duplicates(subset=['userId','movieId'], keep='first', inplace=True)

# Merge data
merge_df = pd.merge(ratings, movies, how='left', on='movieId')
df = merge_df.copy().drop('timestamp', axis=1)
df

# drop missing value dari numpy
df = df[~pd.isnull(df['genres'])]

df.shape

"""## Model Deployment
Model machine learning yang digunakan untuk menyelesaikan permasalahan ini ada 2, yaitu:

* Content Based Filtering
* Collaborative Filtering

### Model Content Based Filtering

#### TF-IDF vectorizer
"""

from sklearn.feature_extraction.text import TfidfVectorizer
# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer(stop_words='english')

# Melakukan perhitungan idf pada data movies
tf.fit(movies['genres']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

"""#### Transform data"""

tfidf_matrix = tf.fit_transform(movies['genres']) 
tfidf_matrix.shape

"""#### Menghitung Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""#### Membuat Dataframe baru menggunakan cosine similarity"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=movies['title'],
                             columns=movies['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix tiap movie
cosine_sim_df.sample(10, axis=1).sample(10, axis=0)

"""### Model Collaborative  Filtering"""

df.head()

"""#### Encode *userId*"""

# Mengubah unique userId menjadi list
user_id = df['userId'].unique().tolist()
print(f'list userId: {user_id}')
 
# Encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_id)}
print(f'\nencoded userId : {user_to_user_encoded}')
 
# Encoding angka ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_id)}

print(f'\nencoded angka ke userId: {user_encoded_to_user}')

"""#### Encode *movieId*"""

# Mengubah unique userId menjadi list
movie_id = df['movieId'].unique().tolist()
print(f'list movieId: {movie_id}')
 
# Encoding userId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_id)}
print(f'\nencoded movieId : {movie_to_movie_encoded}')
 
# Encoding angka ke userId
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_id)}

print(f'\nencoded angka ke movieId: {movie_encoded_to_movie}')

"""#### Mapping"""

# Mapping user_id ke dataframe user
df['user'] = df['userId'].map(user_to_user_encoded)
 
# Mapping movie_id ke dataframe movie
df['movie'] = df['movieId'].map(movie_to_movie_encoded)

df

"""#### Cek jumlah user, jumlah movie, dan mengubah nilai rating menjadi float."""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
 
# Mendapatkan jumlah movie
num_movie = len(movie_encoded_to_movie)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['rating'])
 
# Nilai maksimal rating
max_rating = max(df['rating'])
print(f'Number of User : {num_users}')
print(f'Number of Movie : {num_movie}')
print(f'Min rating : {min_rating}')
print(f'Max rating : {max_rating}')

"""#### Membagi Data untuk Training dan Validasi"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = df[['user', 'movie']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

"""#### Model Training"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.callbacks import  EarlyStopping

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(5e-7)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(5e-7)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4
 
    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2) 
 
    x = dot_user_movie + user_bias + movie_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[[tf.keras.metrics.MeanAbsoluteError(),tf.keras.metrics.RootMeanSquaredError()]]
)
callbacks = EarlyStopping(
    min_delta=0.0001,
    patience=7,
    restore_best_weights=True,
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks=[callbacks]
)

"""#### Rekomendasi Movies dengan Collaborative Filtering"""

# Mengambil sample user
user_ID = df.userId.sample(1).iloc[0]
movie_watched_by_user = df[df.userId == user_ID]
 
movie_not_watched = movies[~movies['movieId'].isin(movie_watched_by_user.movieId.values)]['movieId'] 
movie_not_watched = list(
    set(movie_not_watched)
    .intersection(set(movie_to_movie_encoded.keys()))
)

 
movie_not_watched = [[movie_to_movie_encoded.get(x)] for x in movie_not_watched]
user_encoder = user_to_user_encoded.get(user_ID)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watched), movie_not_watched)
)

ratings = model.predict(user_movie_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_ID))
print('====' * 10)
print('movie with high ratings from user')
print('----' * 8)
 
top_movie_user = (
    movie_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)
 
movie_df_rows = movies[movies['movieId'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title)
 
print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)
 
recommended_movie = movies[movies['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title)

"""## Evaluation
Pada evaluation saya menggunakan dua teknik yaitu :
* Mean Absolute Error (MAE)
* Root Mean Squared Error (RMSE)

Dan untuk Sistem Rekomendasi Content Based Filtering Saya menggunakan `Precision`

### Plot MAE
"""

plt.plot(history.history['mean_absolute_error'])
plt.plot(history.history['val_mean_absolute_error'])
plt.title('model_metrics')
plt.ylabel('mean_absolute_error')
plt.xlabel('epoch')
plt.legend(['mean_absolute_error', 'val_mean_absolute_error'])
plt.show()

"""### Plot RMSE"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['root_mean_squared_error', 'val_root_mean_squared_error'])
plt.show()

"""### Precision Content Based Filtering

#### Membuat Function untuk merekomendasikan 10 movies yang mirip
"""

def MovieRecommendations(movies_title, similarity_data=cosine_sim_df, 
                         items=movies[['movieId','title','genres']], k=10):
  
    ''' Mengambil data menggunakan argpartition untuk partisi secara tidak langsung,
    sepanjang sumbu yang diberikan, kemudian dataframe diubah menjadi numpy Range 
    dengan parameter sebagai berikut (start, stop, step) '''
    index = similarity_data.loc[:, movies_title].to_numpy().argpartition(
        range(-1, -k, -1)
    )

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop movie_title agar nama movie yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(movies_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""##### Rekomendasi movies dengan Content Based Filtering

"""

# Check data lengkap sebuah title 
find_title = movies[movies['title'] == 'Mean Streets (1973)']
find_title

"""> Dapat dilihat bahwa judul film `Mean Streets (1973)` memiliki genre Crime & Drama"""

movie_title = 'Mean Streets (1973)'
movie_recomend = MovieRecommendations(movie_title)
movie_recomend

"""> Dari 10 rekomendasi yang sistem berikan, 10 judul movie tersebut memiliki genre yang sama dengan `Mean Streets (1973)` yaitu Crime & Drama"""